\documentclass{article}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{dirtytalk}
\usepackage{pgfplotstable}
\usepackage{pgfplots}
\usepackage{datatool}
\usepackage{siunitx}
\usepackage[hyphens]{url}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{microtype}
\usepackage{float}
\usepackage[style=ieee]{biblatex}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\usepackage{comment}
\usepackage[normalem]{ulem}
\usepackage[margin=1in]{geometry}
\usepackage{titling}

\setlength{\droptitle}{-1in}
\useunder{\uline}{\ul}{}

\addbibresource{main.bib}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=blue,      
    urlcolor=blue,
    citecolor=blue,
}

\pgfplotsset{compat=1.18}

\title{Individual Plan Master's Thesis\\Computer Science}
\author{Name: Casper Ove Kristiansson\\Email: Casperkr@kth.se}
\date{\today}

\begin{document}

\setlength\parindent{0pt}
\maketitle

\section{Project Information}
\begin{itemize}
  \item \textbf{Preliminary Title:} From Experiment to Insight: A Comparative Study of Storage Approaches for Large-Scale Synchrotron and Neutron Scattering Data on AWS
  \item \textbf{Student:} Casper Ove Kristiansson — \href{mailto:casperkr@kth.se}{casperkr@kth.se}
  \item \textbf{Examiner (KTH):} Prof.\ Mats Nordahl
  \item \textbf{Supervisor (KTH):} Prof.\ Sebastian Dalleiger
  \item \textbf{Industry/External Supervisor:} Dr.\ Ahmet Bahadır Yıldız (Scatterin AB) — \texttt{ahmet@scatterin.com}
  \item \textbf{Current Date:} \date{\today}
  \item \textbf{Keywords:} Synchrotron data; Neutron scattering; Cloud storage; AWS; HDF5; Zarr; Parquet; DynamoDB; Big-data management; Scientific workflows
\end{itemize}

\section{Background \& Objective}
% - Scientific / Societal Context: Petascale data rates at modern synchrotron & neutron facilities; resulting data-management bottlenecks
% - Interest of Organization (Scatterin AB): commercial analytics platform seeks benchmark guidance for AWS back-end
% - High-Level Objective: deliver comparative benchmark & deployment blueprint for four storage approaches on AWS
% - Required Background Knowledge: data-intensive computing, AWS architecture, file formats (HDF5, Zarr, Parquet), full-stack development experience

% <<< Write narrative text here >>>

\section{Research Question \& Method}
% Research Question:
%   How do HDF5, Zarr, Parquet, and DynamoDB compare on AWS for large-scale beamline datasets with respect to latency, throughput, scalability, and cost during partial-slice access?
%
% Hypothesis:
% - Parquet fastest for columnar slices
% - Zarr best for cloud-object parallel I/O
% - HDF5 optimal for dense multi-D patterns
% - DynamoDB strong for high-concurrency meta-queries but costly for large scans
%
% Objectives (measurable):
% - O1 Build 4 equivalent prototypes ingesting ≥ 1 TB real beamline data  
% - O2 Produce metrics under 1–64-client parallel workloads  
% - O3 Compute month-long TCO scenarios for hot/warm/cold access tiers  
% - O4 Publish decision matrix + deployment guide
%
% Tasks & Challenges:
% - Data selection & anonymization — GDPR compliance  
% - Pipeline to transform data into four formats — chunking parity  
% - Benchmark harness on AWS — reproducibility, cost tracking  
% - Analysis & visualization — statistical soundness
%
% Method:
% - Experimental benchmarking; design-science methodology; statistical analysis (e.g.\ ANOVA)
%
% Ethics & Sustainability:
% - Energy use of cloud vs on-prem; include carbon-footprint estimate  
% - Data privacy (beam-time embargo)
%
% Limitations:
% - Focus on AWS only; no tape-archival layer; dataset ≤ 10 TB
%
% Risks & Mitigations:
% - AWS budget overrun → set cost alarms & credits  
% - Insufficient data volume → synthetic data generator fallback  
% - Time slippage → weekly scrum with supervisor

% <<< Write narrative text here >>>

\section{Evaluation \& News Value}
% Evaluation Metrics:
% - Median read/write latency (ms)  
% - Throughput (MB/s)  
% - Cost (\$/GB-month)  
% - Scalability slope (clients vs latency)
%
% Scientific Relevance:
% - Empirical evidence filling gap in HPC-cloud migration literature
%
% Innovation / News Value:
% - First head-to-head benchmark HDF5–Zarr–Parquet–DynamoDB for beamline data  
% - Target readers: facility IT managers, cloud-HPC researchers, AWS scientific computing group

% <<< Write narrative text here >>>

\section{Pre-Study}
% Literature Review Focus:
% - Synchrotron big-data architectures (Wang 2018; ESRF strategy)  
% - Cloud-based beamline portals (Meyer 2014; Moriyama 2019)  
% - HPC I/O libraries & ADIOS2 (Godoy 2021)  
% - Cost-aware cloud storage (AWS white-papers; KEK 2021)
%
% Preliminarily Important References:
% - Wang Y.\ et al., \emph{Small} 2018  
% - Meyer G.R.\ et al., \emph{J.\ Appl.\ Cryst.} 2014  
% - Moriyama T.\ et al., ICALEPCS 2019  
% - ESRF IT Dept.\ 2020  
% - Godoy W.F.\ et al., IEEE Big Data 2021  
% - AWS Case Study: KEK 2021

% <<< Write narrative text here >>>

\section{Conditions \& Schedule}
% Resources Needed:
% - AWS credits \$2 000; S3 > 10 TB; EC2 c6i & r6gd instances; Lambda; Step Functions; CloudWatch  
% - Software: Python, Dask, PyArrow, h5py, zarr-python, AWS SDK  
% - Real beamline datasets (Scatterin AB) + synthetic generator
%
% External Supervisor Involvement:
% - Weekly stand-up (30 min) + Slack daily Q\&A  
% - Monthly checkpoint with KTH supervisor
%
% Timeline (26 weeks):
% - W 1 – 3 Design & dataset acquisition  
% - W 4 – 7 Data transformation pipelines  
% - W 8 – 12 Prototype deployments (4 formats)  
% - W 13 – 17 Benchmarking experiments  
% - W 18 – 20 Cost & sustainability analysis  
% - Week 21 Mid-term presentation  
% - W 22 – 24 Thesis writing & visualizations  
% - W 25 Internal review & revisions  
% - W 26 Final submission & defense

\printbibliography

\end{document}
